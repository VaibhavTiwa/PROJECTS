# -*- coding: utf-8 -*-
"""cutsomjobtemplate

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VmTMQcrsgBPUGfYFkynz2JINtRe1MMyO
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q apache-beam[gcp,interactive]==2.55.1 --no-warn-conflicts

import logging
import re

from IPython.core.display import display, HTML

import apache_beam as beam
from apache_beam import FlatMap, Map, Create
from apache_beam import window, WindowInto
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.combiners import Count
from apache_beam.io.textio import ReadFromText
from apache_beam.io.gcp.pubsub import ReadFromPubSub
from apache_beam.io.gcp.bigquery import WriteToBigQuery

from apache_beam.runners import DataflowRunner
from apache_beam.runners.interactive.interactive_runner import InteractiveRunner
import apache_beam.runners.interactive.interactive_beam as ib

import google.auth
import os

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/vaibhavtiwari-420903-fec43b361cb5.json'

project = google.auth.default()[1]
bucket = f"gs://{project}"
subscription = f"projects/{project}/subscriptions/{project}-sub"
os.environ['PROJECT'] = project
custom_bucket='gs://vaibhtiwari'

options = PipelineOptions(
              project=project,
              region="us-east1",
              temp_location=f"{custom_bucket}/tmp/",
              job_name="customjob",
              cache_location='gs://vaibhtiwari/tmp/'
)
p = beam.Pipeline(DataflowRunner(), options)

# path = "gs://beamapache/data_files/head_usa_names.csv"
# def split_words(text):
#     words = re.findall(r'[\w\']+', text.strip(), re.UNICODE)
#     return [(x, 1) for x in words]

# source = p | ReadFromText(path)

# count = (source | FlatMap(split_words)
#                 | Count.PerKey())

# pipeline = p.run()

"""WRITING DATA FROM GCS TO BQ

"""

# schema = {
#     "fields": [
#         {"name": "id", "type": "STRING"},
#         {"name": "name", "type": "STRING"},
#         {"name": "email", "type": "STRING"},
#         {"name": "age", "type": "STRING"},
#         {"name": "city", "type": "STRING"}
#     ]
# }

# reading=(
#     p
#     |beam.io.ReadFromText('gs://vaibhtiwari/user.csv')
#     |beam.Map(lambda line: dict(zip(('ID', 'Name', 'Email', 'Age', 'City'), line.split(','))))
# )

# writting=(
#     reading| 'WriteToBigQuery' >> WriteToBigQuery(
#         table='vaibhavtiwari-420903.test1.tst-table',  # Change this to your dataset and table name
#         schema=schema,
#         create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
#         write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE
#     )
# )

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions
from apache_beam.io.gcp.bigquery import WriteToBigQuery

# Define project ID, bucket name, and input file path
project_id = 'vaibhavtiwari-420903'
bucket = 'gs://vaibhtiwari'
input_file = 'gs://vaibhtiwari/user.csv'

options = PipelineOptions(
    runner='DataflowRunner',
    cache_location='gs://vaibhtiwari/tmp/'  # Change this only if you cannot grant write access
)
google_cloud_options = options.view_as(GoogleCloudOptions)
google_cloud_options.project = project_id
google_cloud_options.region = 'us-central1'


schema = {
    "fields": [
        {"name": "id", "type": "STRING"},
        {"name": "name", "type": "STRING"},
        {"name": "email", "type": "STRING"},
        {"name": "age", "type": "STRING"},
        {"name": "city", "type": "STRING"}
    ]
}


with beam.Pipeline(options=options) as pipeline:
    # Read the CSV file from GCS
    lines = pipeline | 'ReadFromGCS' >> beam.io.ReadFromText(input_file)

    
    data = lines | 'ParseCSV' >> beam.Map(lambda line: dict(zip(('ID', 'Name', 'Email', 'Age', 'City'), line.split(','))))

   
    data | 'WriteToBigQuery' >> WriteToBigQuery(
        table='vaibhavtiwari-420903.test1.tst-table',  # Change to your dataset and table name
        schema=schema,
        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
        write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE
    )
